<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI on Blog</title><link>https://analyst-huang.github.io/posts/ai/</link><description>Recent content in AI on Blog</description><generator>Hugo -- 0.154.1</generator><language>zh-cn</language><lastBuildDate>Fri, 02 Jan 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://analyst-huang.github.io/posts/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>ELBO：证据、隐变量与变分下界的统一视角</title><link>https://analyst-huang.github.io/posts/ai/elbo/</link><pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate><guid>https://analyst-huang.github.io/posts/ai/elbo/</guid><description>&lt;h1 id="elbo证据隐变量与变分下界的统一视角"&gt;ELBO：证据、隐变量与变分下界的统一视角&lt;/h1&gt;
&lt;p&gt;本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;训练目标始终是最大化 &lt;strong&gt;证据&lt;/strong&gt;（边缘似然）。&lt;/li&gt;
&lt;li&gt;隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。&lt;/li&gt;
&lt;li&gt;ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="1-证据到底是什么"&gt;1. “证据”到底是什么&lt;/h2&gt;
&lt;p&gt;给定观测数据 $x$ 与生成模型参数 $\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率：
$p_\theta(x)$&lt;/p&gt;
&lt;p&gt;它也常被称为 &lt;strong&gt;边缘似然（marginal likelihood）&lt;/strong&gt;、&lt;strong&gt;模型证据（model evidence）&lt;/strong&gt;。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：&lt;/p&gt;
&lt;span class="math" style="display: block; text-align: center;"&gt;
$p_\theta(x)=\int p_\theta(x,z)\,dz$
&lt;/span&gt;
&lt;p&gt;如果进一步将联合分布写成“先验 + 条件似然”的形式：&lt;/p&gt;
&lt;span class="math" style="display: block; text-align: center;"&gt;
$p_\theta(x,z)=p_\theta(x\mid z)\,p(z)$
&lt;/span&gt;
&lt;p&gt;则证据变为：&lt;/p&gt;
&lt;span class="math" style="display: block; text-align: center;"&gt;
$p_\theta(x)=\int p_\theta(x\mid z)\,p(z)\,dz$
&lt;/span&gt;
&lt;p&gt;这句话的统计含义非常直接：&lt;strong&gt;模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力&lt;/strong&gt;。在贝叶斯公式中，它是后验归一化因子：&lt;/p&gt;
&lt;span class="math" style="display: block; text-align: center;"&gt;
$p_\theta(z\mid x)=\frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$
&lt;/span&gt;
&lt;p&gt;因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是&lt;strong&gt;所有可能解释对 $x$ 的总体支持度&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="2-隐变量在统计中的地位不是工程权宜而是核心范式"&gt;2. 隐变量在统计中的地位：不是工程权宜，而是核心范式&lt;/h2&gt;
&lt;p&gt;隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类：&lt;/p&gt;</description></item><item><title>PPO：策略梯度、重要性采样与 Clip</title><link>https://analyst-huang.github.io/posts/ai/ppo/</link><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate><guid>https://analyst-huang.github.io/posts/ai/ppo/</guid><description>&lt;hr&gt;
&lt;h1 id="ppo-中的策略梯度重要性采样与概率密度比"&gt;PPO 中的策略梯度、重要性采样与概率密度比&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;从策略梯度定理出发，经由重要性采样与“对梯度的不定积分”，理解 PPO 的 surrogate objective 与 Clip 机制。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id="1-策略梯度定理起点而不是终点"&gt;1. 策略梯度定理：起点而不是终点&lt;/h2&gt;
&lt;p&gt;策略梯度定理给出的是&lt;strong&gt;梯度形式&lt;/strong&gt;，而不是一个可直接优化的损失函数：&lt;/p&gt;
&lt;span class="math" style="display: block; text-align: center;"&gt;
$$
\nabla_\theta J(\theta)
= \mathbb E_{\pi_\theta}
\big[
\nabla_\theta \log \pi_\theta(a_t\mid s_t), A^\pi(s_t,a_t)
\big].
$$
&lt;/span&gt;
&lt;p&gt;这一定理说明了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;梯度方向由 &lt;span class="math" style="display: block; text-align: center;"&gt;
$ \nabla_\theta \log \pi $
&lt;/span&gt;
决定；&lt;/li&gt;
&lt;li&gt;学习信号由优势函数 &lt;span class="math" style="display: block; text-align: center;"&gt;
$A$
&lt;/span&gt;
提供；&lt;/li&gt;
&lt;li&gt;它本质上是 &lt;strong&gt;on-policy&lt;/strong&gt; 的结论。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但工程上我们并不是直接“写梯度”，而是希望：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;构造一个目标函数 &lt;span class="math" style="display: block; text-align: center;"&gt;
$L(\theta)$
&lt;/span&gt;
，使其梯度自动给出合理的策略更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这正是 PPO / TRPO 所做的事情。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="2-从-on-policy-到-off-policy重要性采样的引入"&gt;2. 从 on-policy 到 off-policy：重要性采样的引入&lt;/h2&gt;
&lt;p&gt;在实践中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rollout 使用的是旧策略 &lt;span class="math" style="display: block; text-align: center;"&gt;
$ \pi_{\text{old}} $
&lt;/span&gt;
；&lt;/li&gt;
&lt;li&gt;更新时参数已经变为 &lt;span class="math" style="display: block; text-align: center;"&gt;
$ \pi_\theta $
&lt;/span&gt;
。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是需要通过重要性采样来修正期望的测度：&lt;/p&gt;</description></item><item><title>策略梯度定理</title><link>https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/</link><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate><guid>https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/</guid><description>&lt;h1 id="策略梯度从-reinforce-到策略梯度定理含详细证明"&gt;策略梯度：从 REINFORCE 到策略梯度定理（含详细证明）&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;本文目标：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质；&lt;/li&gt;
&lt;li&gt;给出 REINFORCE（Williams, 1992）的 &lt;strong&gt;score-function / likelihood-ratio&lt;/strong&gt; 推导；&lt;/li&gt;
&lt;li&gt;给出 Sutton et al.（1999）式 &lt;strong&gt;策略梯度定理（Policy Gradient Theorem）&lt;/strong&gt; 的 Bellman/占用测度推导；&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id="0-记号与设定"&gt;0. 记号与设定&lt;/h2&gt;
&lt;p&gt;我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;状态：$s\in\mathcal S$&lt;/li&gt;
&lt;li&gt;动作：$a\in\mathcal A$&lt;/li&gt;
&lt;li&gt;转移：$P(s&amp;rsquo;\mid s,a)$&lt;/li&gt;
&lt;li&gt;即时奖励：$r(s,a)$（或 $r_t$）&lt;/li&gt;
&lt;li&gt;折扣：$\gamma\in(0,1)$&lt;/li&gt;
&lt;li&gt;初始状态分布：$\mu(s)=\Pr(s_0=s)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;随机策略以参数 $\theta$ 参数化：$\pi_\theta(a\mid s)$。&lt;/p&gt;
&lt;p&gt;定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：&lt;/p&gt;
&lt;p&gt;$$
R(\tau):=\sum_{t=0}^{T}\gamma^t r_{t+1}.
$$&lt;/p&gt;
&lt;p&gt;目标函数（期望折扣回报）：&lt;/p&gt;
&lt;p&gt;$$
J(\theta)=\mathbb E_{\tau\sim p_\theta}[R(\tau)].
$$&lt;/p&gt;
&lt;p&gt;其中轨迹 $\tau$ 表示
$
\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_T,a_T,r_{T+1})
$
，轨迹分布为&lt;/p&gt;
&lt;p&gt;$$
p_\theta(\tau)=\mu(s_0)\prod_{t=0}^{T}\pi_\theta(a_t\mid s_t),P(s_{t+1}\mid s_t,a_t).
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;关键点：环境动力学 $P$ 与初始分布 $\mu$ 不依赖 $\theta$。$\theta$ 只通过策略 $\pi_\theta$ 进入。&lt;/p&gt;</description></item></channel></rss>