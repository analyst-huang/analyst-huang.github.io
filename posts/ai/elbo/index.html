<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ELBO：证据、隐变量与变分下界的统一视角 | Blog</title><meta name=keywords content><meta name=description content="ELBO：证据、隐变量与变分下界的统一视角
本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：

训练目标始终是最大化 证据（边缘似然）。
隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。
ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。


1. “证据”到底是什么
给定观测数据 $x$ 与生成模型参数 $\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率：
$p_\theta(x)$
它也常被称为 边缘似然（marginal likelihood）、模型证据（model evidence）。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：

  
$p_\theta(x)=\int p_\theta(x,z)\,dz$



如果进一步将联合分布写成“先验 + 条件似然”的形式：

  
$p_\theta(x,z)=p_\theta(x\mid z)\,p(z)$



则证据变为：

  
$p_\theta(x)=\int p_\theta(x\mid z)\,p(z)\,dz$



这句话的统计含义非常直接：模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力。在贝叶斯公式中，它是后验归一化因子：

  
$p_\theta(z\mid x)=\frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$



因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是所有可能解释对 $x$ 的总体支持度。

2. 隐变量在统计中的地位：不是工程权宜，而是核心范式
隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类："><meta name=author content><link rel=canonical href=https://analyst-huang.github.io/posts/ai/elbo/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://analyst-huang.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://analyst-huang.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://analyst-huang.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://analyst-huang.github.io/apple-touch-icon.png><link rel=mask-icon href=https://analyst-huang.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://analyst-huang.github.io/posts/ai/elbo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\\\(",right:"\\\\)",display:!1},{left:"\\\\[",right:"\\\\]",display:!0}]})})</script><meta property="og:url" content="https://analyst-huang.github.io/posts/ai/elbo/"><meta property="og:site_name" content="Blog"><meta property="og:title" content="ELBO：证据、隐变量与变分下界的统一视角"><meta property="og:description" content="ELBO：证据、隐变量与变分下界的统一视角 本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：
训练目标始终是最大化 证据（边缘似然）。 隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。 ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。 1. “证据”到底是什么 给定观测数据 $x$ 与生成模型参数 $\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率： $p_\theta(x)$
它也常被称为 边缘似然（marginal likelihood）、模型证据（model evidence）。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：
$p_\theta(x)=\int p_\theta(x,z)\,dz$ 如果进一步将联合分布写成“先验 + 条件似然”的形式：
$p_\theta(x,z)=p_\theta(x\mid z)\,p(z)$ 则证据变为：
$p_\theta(x)=\int p_\theta(x\mid z)\,p(z)\,dz$ 这句话的统计含义非常直接：模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力。在贝叶斯公式中，它是后验归一化因子：
$p_\theta(z\mid x)=\frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$ 因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是所有可能解释对 $x$ 的总体支持度。
2. 隐变量在统计中的地位：不是工程权宜，而是核心范式 隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类："><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-02T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="ELBO：证据、隐变量与变分下界的统一视角"><meta name=twitter:description content="ELBO：证据、隐变量与变分下界的统一视角
本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：

训练目标始终是最大化 证据（边缘似然）。
隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。
ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。


1. “证据”到底是什么
给定观测数据 $x$ 与生成模型参数 $\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率：
$p_\theta(x)$
它也常被称为 边缘似然（marginal likelihood）、模型证据（model evidence）。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：

  
$p_\theta(x)=\int p_\theta(x,z)\,dz$



如果进一步将联合分布写成“先验 + 条件似然”的形式：

  
$p_\theta(x,z)=p_\theta(x\mid z)\,p(z)$



则证据变为：

  
$p_\theta(x)=\int p_\theta(x\mid z)\,p(z)\,dz$



这句话的统计含义非常直接：模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力。在贝叶斯公式中，它是后验归一化因子：

  
$p_\theta(z\mid x)=\frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$



因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是所有可能解释对 $x$ 的总体支持度。

2. 隐变量在统计中的地位：不是工程权宜，而是核心范式
隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://analyst-huang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI","item":"https://analyst-huang.github.io/posts/ai/"},{"@type":"ListItem","position":3,"name":"ELBO：证据、隐变量与变分下界的统一视角","item":"https://analyst-huang.github.io/posts/ai/elbo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ELBO：证据、隐变量与变分下界的统一视角","name":"ELBO：证据、隐变量与变分下界的统一视角","description":"ELBO：证据、隐变量与变分下界的统一视角 本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：\n训练目标始终是最大化 证据（边缘似然）。 隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。 ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。 1. “证据”到底是什么 给定观测数据 $x$ 与生成模型参数 $\\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率： $p_\\theta(x)$\n它也常被称为 边缘似然（marginal likelihood）、模型证据（model evidence）。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：\n$p_\\theta(x)=\\int p_\\theta(x,z)\\,dz$ 如果进一步将联合分布写成“先验 + 条件似然”的形式：\n$p_\\theta(x,z)=p_\\theta(x\\mid z)\\,p(z)$ 则证据变为：\n$p_\\theta(x)=\\int p_\\theta(x\\mid z)\\,p(z)\\,dz$ 这句话的统计含义非常直接：模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力。在贝叶斯公式中，它是后验归一化因子：\n$p_\\theta(z\\mid x)=\\frac{p_\\theta(x\\mid z)p(z)}{p_\\theta(x)}$ 因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是所有可能解释对 $x$ 的总体支持度。\n2. 隐变量在统计中的地位：不是工程权宜，而是核心范式 隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类：\n","keywords":[],"articleBody":"ELBO：证据、隐变量与变分下界的统一视角 本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：\n训练目标始终是最大化 证据（边缘似然）。 隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。 ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。 1. “证据”到底是什么 给定观测数据 $x$ 与生成模型参数 $\\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率： $p_\\theta(x)$\n它也常被称为 边缘似然（marginal likelihood）、模型证据（model evidence）。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：\n$p_\\theta(x)=\\int p_\\theta(x,z)\\,dz$ 如果进一步将联合分布写成“先验 + 条件似然”的形式：\n$p_\\theta(x,z)=p_\\theta(x\\mid z)\\,p(z)$ 则证据变为：\n$p_\\theta(x)=\\int p_\\theta(x\\mid z)\\,p(z)\\,dz$ 这句话的统计含义非常直接：模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力。在贝叶斯公式中，它是后验归一化因子：\n$p_\\theta(z\\mid x)=\\frac{p_\\theta(x\\mid z)p(z)}{p_\\theta(x)}$ 因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是所有可能解释对 $x$ 的总体支持度。\n2. 隐变量在统计中的地位：不是工程权宜，而是核心范式 隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类：\n2.1 结构建模：世界里确实有不可观测的量 典型例子包括：\n混合模型：数据来自多个未观测群体（类别标签不可见）。 因子分析：观测维度相关来自共享的少数潜因子。 状态空间模型 / HMM：系统有不可见状态，观测只是状态的噪声投影。 随机效应 / 分层模型：个体差异（或组间差异）作为潜变量进入模型。 这些模型并不是为了“让概率可拆解”，而是为了让模型表达能力与科学解释更贴近现实机制。\n2.2 计算装置：data augmentation 与 complete-data trick 即便隐变量不具备强语义，它也常被当作“计算装置”：直接处理边缘似然 $p_\\theta(x)$ 很难，但“假装看到了完整数据 $(x,z)$”后，联合似然 $p_\\theta(x,z)$ 往往更易处理。EM 算法正是这一思想的经典实例；而 ELBO 与 EM 在数学上存在直接对应（见第 6 节）。\n3. 为什么需要 ELBO：证据难算、难优化 最大似然学习的目标并没有因为引入潜变量而改变。对单样本（或数据集求和）而言，目标仍是最大化对数证据：\n$\\max_\\theta \\log p_\\theta(x)$ 困难在于：\n$\\log p_\\theta(x)=\\log\\int p_\\theta(x,z)\\,dz$ 高维连续潜空间下该积分通常不可解析；即便使用蒙特卡洛，梯度估计方差也会很高。ELBO 的作用是把这个不可直接优化的目标，变成一个可计算、可微、可用随机梯度优化的下界目标。\n4. ELBO 的推导：从证据到下界 引入任意分布 $q_\\phi(z\\mid x)$（变分分布或推断网络），做一个恒等变形：\n$\\log p_\\theta(x) = \\log \\int q_\\phi(z\\mid x)\\frac{p_\\theta(x,z)}{q_\\phi(z\\mid x)}\\,dz$ 由于对数函数凹，使用 Jensen 不等式得到下界：\n$\\log p_\\theta(x) \\ge \\mathbb E_{q_\\phi(z\\mid x)} \\left[ \\log p_\\theta(x,z)-\\log q_\\phi(z\\mid x) \\right]$ 定义右侧为 ELBO：\n$\\mathcal L(\\theta,\\phi) = \\mathbb E_{q_\\phi(z\\mid x)} \\left[ \\log p_\\theta(x,z)-\\log q_\\phi(z\\mid x) \\right]$ 展开联合分布 $\\log p_\\theta(x,z)=\\log p_\\theta(x\\mid z)+\\log p(z)$ 后得到最常用形式：\n$\\mathcal L(\\theta,\\phi) = \\mathbb E_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - \\mathrm{KL}\\bigl(q_\\phi(z\\mid x)\\,\\|\\,p(z)\\bigr)$ 5. 两个 KL：一个在 ELBO 里，一个是 ELBO 与证据的缺口 一个常见困惑是：ELBO 里为何出现的是\n$-\\mathrm{KL}\\bigl(q_\\phi(z\\mid x)\\,\\|\\,p(z)\\bigr)$ 而不是\n$-\\mathrm{KL}\\bigl(q_\\phi(z\\mid x)\\,\\|\\,p_\\theta(z\\mid x)\\bigr)$ 关键在于：后者确实与 ELBO 紧密相关，但它不作为“显式项”出现，而是作为 **缺口（gap）**精确地刻画 ELBO 的松紧：\n$\\log p_\\theta(x) = \\mathcal L(\\theta,\\phi) + \\mathrm{KL}\\bigl(q_\\phi(z\\mid x)\\,\\|\\,p_\\theta(z\\mid x)\\bigr)$ 因此：\nELBO 永远是下界，因为 KL 非负。 下界变紧当且仅当 $q_\\phi(z\\mid x)=p_\\theta(z\\mid x)$。 更重要的是，当 $\\theta$ 固定时，$\\log p_\\theta(x)$ 与 $\\phi$ 无关，因此最大化 ELBO 等价于最小化该缺口：\n$\\arg\\max_\\phi \\mathcal L(\\theta,\\phi) = \\arg\\min_\\phi \\mathrm{KL}\\bigl(q_\\phi(z\\mid x)\\,\\|\\,p_\\theta(z\\mid x)\\bigr)$ 附录：引入变分分布是否只是技术手段？ 在 ELBO 的推导中，变分分布 $q_\\phi(z\\mid x)$ 往往以如下方式出现：\n为了处理难以计算的边缘似然 $\\log p_\\theta(x)=\\log\\int p_\\theta(x,z)\\,dz$ 人为引入一个分布并利用 Jensen 不等式构造下界。从形式上看，这一步确实带有明显的“技术性”色彩。\n但如果仅将其理解为数学技巧，就会遗漏变分推断在统计学中更根本的含义。\n一、计算动机：它解决了“边缘化不可计算”的问题 在含隐变量模型中，直接优化 $\\log p_\\theta(x)$ 通常不可行。引入 $q(z\\mid x)$ 后，可以将对数积分改写为期望形式，并得到 ELBO：\n$\\mathcal L(\\theta,q) = \\mathbb E_{q(z\\mid x)}[\\log p_\\theta(x,z)-\\log q(z\\mid x)]$ 在这一层面上，$q$ 的确是一个为可计算性服务的辅助对象。但这并不能解释：\n为什么 $q$ 被视为“后验近似”，而不是任意权重函数。\n二、统计解释：$q(z\\mid x)$ 是一个近似后验，而非随意分布 关键事实在于，ELBO 与证据之间存在如下精确恒等分解：\n$\\log p_\\theta(x) = \\mathcal L(\\theta,q) + \\mathrm{KL}\\bigl(q(z\\mid x)\\,\\|\\,p_\\theta(z\\mid x)\\bigr)$ 这一定义并非事后附会，而是直接来自概率恒等式。其含义是：\nELBO 与真实目标（证据）的差距，完全由 $q$ 与真实后验的 KL 决定； 当 $\\theta$ 固定时，最大化 ELBO 等价于最小化 $\\mathrm{KL}\\bigl(q(z\\mid x)\\,\\|\\,p_\\theta(z\\mid x)\\bigr)$ 因此，在统计意义上，引入 $q(z\\mid x)$ 并不是随意的：\n它是在你允许的分布族中，选择一个最接近真实后验的、可计算的近似。\n这正是变分推断（variational inference）作为推断方法的核心定义。\n三、为什么 ELBO 中出现的是 $\\mathrm{KL}(q|p(z))$？ 在 ELBO 的常见展开形式中，显式出现的是：\n$\\mathbb E_{q(z\\mid x)}[\\log p_\\theta(x\\mid z)] - \\mathrm{KL}\\bigl(q(z\\mid x)\\,\\|\\,p(z)\\bigr)$ 而不是直接出现 $\\mathrm{KL}\\bigl(q(z\\mid x),|,p_\\theta(z\\mid x)\\bigr)$。这是因为：\n后者已经被“吸收”进证据与 ELBO 的差额之中； 前者将后验近似与先验结构绑定，从而保证： 潜变量表示可采样； 学到的表示在整体生成模型下是自洽的。 这并不是改变目标，而是将“逼近真实后验”和“保证生成一致性”这两件事分解为显式项与隐含项。\n小结 引入变分分布 $q(z\\mid x)$ 的确始于计算需求，但其合理性并不止于技巧层面：\n它在统计上对应一个明确的后验近似问题； ELBO 精确刻画了该近似与真实后验之间的差距； 在现代 AI 中，$q$ 被进一步参数化并摊销，从而成为可扩展的推断机制。 因此，更准确的说法是：\n变分分布不是“为了 ELBO 而引入的技巧”， 而是 ELBO 所对应的近似推断立场本身。 是我们在认识论上允许自己用什么结构来认识分布。\n","wordCount":"342","inLanguage":"en","datePublished":"2026-01-02T00:00:00Z","dateModified":"2026-01-02T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://analyst-huang.github.io/posts/ai/elbo/"},"publisher":{"@type":"Organization","name":"Blog","logo":{"@type":"ImageObject","url":"https://analyst-huang.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://analyst-huang.github.io/ accesskey=h title="Blog (Alt + H)">Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://analyst-huang.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://analyst-huang.github.io/about/ title=关于><span>关于</span></a></li><li><a href=https://analyst-huang.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://analyst-huang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://analyst-huang.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://analyst-huang.github.io/posts/ai/>AI</a></div><h1 class="post-title entry-hint-parent">ELBO：证据、隐变量与变分下界的统一视角</h1><div class=post-meta><span title='2026-01-02 00:00:00 +0000 UTC'>January 2, 2026</span>&nbsp;·&nbsp;<span>2 min</span></div></header><div class=post-content><h1 id=elbo证据隐变量与变分下界的统一视角>ELBO：证据、隐变量与变分下界的统一视角<a hidden class=anchor aria-hidden=true href=#elbo证据隐变量与变分下界的统一视角>#</a></h1><p>本文给出一个可重复推导、可迁移到多种 AI 场景（VAE、世界模型、序列潜变量模型、变分推断等）的 ELBO（Evidence Lower Bound）理解框架。核心主线是：</p><ol><li>训练目标始终是最大化 <strong>证据</strong>（边缘似然）。</li><li>隐变量不是深度学习才有的“工程拆解”，而是统计建模与推断的长期核心工具；ELBO 则是经典“下界化 + 可优化”范式的现代实现。</li><li>ELBO 中显式出现的先验 KL 与“逼近真实后验”的 KL 并不矛盾：前者是目标函数的结构项，后者是 ELBO 与证据之间的缺口（gap）。</li></ol><hr><h2 id=1-证据到底是什么>1. “证据”到底是什么<a hidden class=anchor aria-hidden=true href=#1-证据到底是什么>#</a></h2><p>给定观测数据 $x$ 与生成模型参数 $\theta$，所谓 **证据（evidence）**是数据在模型下出现的概率：
$p_\theta(x)$</p><p>它也常被称为 <strong>边缘似然（marginal likelihood）</strong>、<strong>模型证据（model evidence）</strong>。当模型含潜变量 $z$ 时，证据是对潜变量积分（或求和）后的量：</p><span class=math style=display:block;text-align:center>$p_\theta(x)=\int p_\theta(x,z)\,dz$</span><p>如果进一步将联合分布写成“先验 + 条件似然”的形式：</p><span class=math style=display:block;text-align:center>$p_\theta(x,z)=p_\theta(x\mid z)\,p(z)$</span><p>则证据变为：</p><span class=math style=display:block;text-align:center>$p_\theta(x)=\int p_\theta(x\mid z)\,p(z)\,dz$</span><p>这句话的统计含义非常直接：<strong>模型整体（在不知道真实潜变量的情况下）生成 $x$ 的能力</strong>。在贝叶斯公式中，它是后验归一化因子：</p><span class=math style=display:block;text-align:center>$p_\theta(z\mid x)=\frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$</span><p>因此，“证据”并不是某个特定解释 $z^*$ 的质量，而是<strong>所有可能解释对 $x$ 的总体支持度</strong>。</p><hr><h2 id=2-隐变量在统计中的地位不是工程权宜而是核心范式>2. 隐变量在统计中的地位：不是工程权宜，而是核心范式<a hidden class=anchor aria-hidden=true href=#2-隐变量在统计中的地位不是工程权宜而是核心范式>#</a></h2><p>隐变量（latent variables）在统计中长期处于中心位置，原因主要有两类：</p><h3 id=21-结构建模世界里确实有不可观测的量>2.1 结构建模：世界里确实有不可观测的量<a hidden class=anchor aria-hidden=true href=#21-结构建模世界里确实有不可观测的量>#</a></h3><p>典型例子包括：</p><ul><li><strong>混合模型</strong>：数据来自多个未观测群体（类别标签不可见）。</li><li><strong>因子分析</strong>：观测维度相关来自共享的少数潜因子。</li><li><strong>状态空间模型 / HMM</strong>：系统有不可见状态，观测只是状态的噪声投影。</li><li><strong>随机效应 / 分层模型</strong>：个体差异（或组间差异）作为潜变量进入模型。</li></ul><p>这些模型并不是为了“让概率可拆解”，而是为了让模型表达能力与科学解释更贴近现实机制。</p><h3 id=22-计算装置data-augmentation-与-complete-data-trick>2.2 计算装置：data augmentation 与 complete-data trick<a hidden class=anchor aria-hidden=true href=#22-计算装置data-augmentation-与-complete-data-trick>#</a></h3><p>即便隐变量不具备强语义，它也常被当作“计算装置”：直接处理边缘似然 $p_\theta(x)$ 很难，但“假装看到了完整数据 $(x,z)$”后，联合似然 $p_\theta(x,z)$ 往往更易处理。EM 算法正是这一思想的经典实例；而 ELBO 与 EM 在数学上存在直接对应（见第 6 节）。</p><hr><h2 id=3-为什么需要-elbo证据难算难优化>3. 为什么需要 ELBO：证据难算、难优化<a hidden class=anchor aria-hidden=true href=#3-为什么需要-elbo证据难算难优化>#</a></h2><p>最大似然学习的目标并没有因为引入潜变量而改变。对单样本（或数据集求和）而言，目标仍是最大化对数证据：</p><span class=math style=display:block;text-align:center>$\max_\theta \log p_\theta(x)$</span><p>困难在于：</p><span class=math style=display:block;text-align:center>$\log p_\theta(x)=\log\int p_\theta(x,z)\,dz$</span><p>高维连续潜空间下该积分通常不可解析；即便使用蒙特卡洛，梯度估计方差也会很高。ELBO 的作用是把这个不可直接优化的目标，变成一个可计算、可微、可用随机梯度优化的下界目标。</p><hr><h2 id=4-elbo-的推导从证据到下界>4. ELBO 的推导：从证据到下界<a hidden class=anchor aria-hidden=true href=#4-elbo-的推导从证据到下界>#</a></h2><p>引入任意分布 $q_\phi(z\mid x)$（变分分布或推断网络），做一个恒等变形：</p><span class=math style=display:block;text-align:center>$\log p_\theta(x)
=
\log \int q_\phi(z\mid x)\frac{p_\theta(x,z)}{q_\phi(z\mid x)}\,dz$</span><p>由于对数函数凹，使用 Jensen 不等式得到下界：</p><span class=math style=display:block;text-align:center>$\log p_\theta(x)
\ge
\mathbb E_{q_\phi(z\mid x)}
\left[
\log p_\theta(x,z)-\log q_\phi(z\mid x)
\right]$</span><p>定义右侧为 ELBO：</p><span class=math style=display:block;text-align:center>$\mathcal L(\theta,\phi)
=
\mathbb E_{q_\phi(z\mid x)}
\left[
\log p_\theta(x,z)-\log q_\phi(z\mid x)
\right]$</span><p>展开联合分布 $\log p_\theta(x,z)=\log p_\theta(x\mid z)+\log p(z)$ 后得到最常用形式：</p><span class=math style=display:block;text-align:center>$\mathcal L(\theta,\phi)
=
\mathbb E_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
-
\mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p(z)\bigr)$</span><hr><h2 id=5-两个-kl一个在-elbo-里一个是-elbo-与证据的缺口>5. 两个 KL：一个在 ELBO 里，一个是 ELBO 与证据的缺口<a hidden class=anchor aria-hidden=true href=#5-两个-kl一个在-elbo-里一个是-elbo-与证据的缺口>#</a></h2><p>一个常见困惑是：ELBO 里为何出现的是</p><span class=math style=display:block;text-align:center>$-\mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p(z)\bigr)$</span><p>而不是</p><span class=math style=display:block;text-align:center>$-\mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\bigr)$</span><p>关键在于：后者确实与 ELBO 紧密相关，但它不作为“显式项”出现，而是作为 **缺口（gap）**精确地刻画 ELBO 的松紧：</p><span class=math style=display:block;text-align:center>$\log p_\theta(x)
=
\mathcal L(\theta,\phi)
+
\mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\bigr)$</span><p>因此：</p><ul><li>ELBO 永远是下界，因为 KL 非负。</li><li>下界变紧当且仅当 $q_\phi(z\mid x)=p_\theta(z\mid x)$。</li></ul><p>更重要的是，当 $\theta$ 固定时，$\log p_\theta(x)$ 与 $\phi$ 无关，因此最大化 ELBO 等价于最小化该缺口：</p><span class=math style=display:block;text-align:center>$\arg\max_\phi \mathcal L(\theta,\phi)
=
\arg\min_\phi \mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\bigr)$</span><hr><hr><h2 id=附录引入变分分布是否只是技术手段>附录：引入变分分布是否只是技术手段？<a hidden class=anchor aria-hidden=true href=#附录引入变分分布是否只是技术手段>#</a></h2><p>在 ELBO 的推导中，变分分布 $q_\phi(z\mid x)$ 往往以如下方式出现：<br>为了处理难以计算的边缘似然
<span class=math style=display:block;text-align:center>$\log p_\theta(x)=\log\int p_\theta(x,z)\,dz$
</span>人为引入一个分布并利用 Jensen 不等式构造下界。从形式上看，这一步确实带有明显的“技术性”色彩。</p><p>但如果仅将其理解为数学技巧，就会遗漏变分推断在统计学中更根本的含义。</p><hr><h3 id=一计算动机它解决了边缘化不可计算的问题>一、计算动机：它解决了“边缘化不可计算”的问题<a hidden class=anchor aria-hidden=true href=#一计算动机它解决了边缘化不可计算的问题>#</a></h3><p>在含隐变量模型中，直接优化 $\log p_\theta(x)$ 通常不可行。引入 $q(z\mid x)$ 后，可以将对数积分改写为期望形式，并得到 ELBO：</p><span class=math style=display:block;text-align:center>$\mathcal L(\theta,q)
=
\mathbb E_{q(z\mid x)}[\log p_\theta(x,z)-\log q(z\mid x)]$</span><p>在这一层面上，$q$ 的确是一个<strong>为可计算性服务的辅助对象</strong>。但这并不能解释：<br>为什么 $q$ 被视为“后验近似”，而不是任意权重函数。</p><hr><h3 id=二统计解释qzmid-x-是一个近似后验而非随意分布>二、统计解释：$q(z\mid x)$ 是一个近似后验，而非随意分布<a hidden class=anchor aria-hidden=true href=#二统计解释qzmid-x-是一个近似后验而非随意分布>#</a></h3><p>关键事实在于，ELBO 与证据之间存在如下<strong>精确恒等分解</strong>：</p><span class=math style=display:block;text-align:center>$\log p_\theta(x)
=
\mathcal L(\theta,q)
+
\mathrm{KL}\bigl(q(z\mid x)\,\|\,p_\theta(z\mid x)\bigr)$</span><p>这一定义并非事后附会，而是直接来自概率恒等式。其含义是：</p><ul><li>ELBO 与真实目标（证据）的差距，<strong>完全由 $q$ 与真实后验的 KL 决定</strong>；</li><li>当 $\theta$ 固定时，最大化 ELBO 等价于最小化
<span class=math style=display:block;text-align:center>$\mathrm{KL}\bigl(q(z\mid x)\,\|\,p_\theta(z\mid x)\bigr)$</span></li></ul><p>因此，在统计意义上，引入 $q(z\mid x)$ 并不是随意的：</p><blockquote><p>它是在你允许的分布族中，选择一个<strong>最接近真实后验的、可计算的近似</strong>。</p></blockquote><p>这正是变分推断（variational inference）作为推断方法的核心定义。</p><hr><h3 id=三为什么-elbo-中出现的是-mathrmklqpz>三、为什么 ELBO 中出现的是 $\mathrm{KL}(q|p(z))$？<a hidden class=anchor aria-hidden=true href=#三为什么-elbo-中出现的是-mathrmklqpz>#</a></h3><p>在 ELBO 的常见展开形式中，显式出现的是：</p><span class=math style=display:block;text-align:center>$\mathbb E_{q(z\mid x)}[\log p_\theta(x\mid z)]
-
\mathrm{KL}\bigl(q(z\mid x)\,\|\,p(z)\bigr)$</span><p>而不是直接出现 $\mathrm{KL}\bigl(q(z\mid x),|,p_\theta(z\mid x)\bigr)$。这是因为：</p><ul><li>后者已经被“吸收”进证据与 ELBO 的差额之中；</li><li>前者将后验近似与先验结构绑定，从而保证：<ul><li>潜变量表示可采样；</li><li>学到的表示在整体生成模型下是自洽的。</li></ul></li></ul><p>这并不是改变目标，而是将“逼近真实后验”和“保证生成一致性”这两件事分解为显式项与隐含项。</p><hr><h3 id=小结>小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h3><p>引入变分分布 $q(z\mid x)$ 的确始于计算需求，但其合理性并不止于技巧层面：</p><ul><li>它在统计上对应一个明确的后验近似问题；</li><li>ELBO 精确刻画了该近似与真实后验之间的差距；</li><li>在现代 AI 中，$q$ 被进一步参数化并摊销，从而成为可扩展的推断机制。</li></ul><p>因此，更准确的说法是：</p><blockquote><p><strong>变分分布不是“为了 ELBO 而引入的技巧”，<br>而是 ELBO 所对应的近似推断立场本身。<br>是我们在认识论上允许自己用什么结构来认识分布。</strong></p></blockquote><hr></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://analyst-huang.github.io/>Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>