<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>策略梯度定理 | Blog</title><meta name=keywords content><meta name=description content="策略梯度：从 REINFORCE 到策略梯度定理（含详细证明）

本文目标：

用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质；
给出 REINFORCE（Williams, 1992）的 score-function / likelihood-ratio 推导；
给出 Sutton et al.（1999）式 策略梯度定理（Policy Gradient Theorem） 的 Bellman/占用测度推导；



0. 记号与设定
我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：

状态：$s\in\mathcal S$
动作：$a\in\mathcal A$
转移：$P(s&rsquo;\mid s,a)$
即时奖励：$r(s,a)$（或 $r_t$）
折扣：$\gamma\in(0,1)$
初始状态分布：$\mu(s)=\Pr(s_0=s)$

随机策略以参数 $\theta$ 参数化：$\pi_\theta(a\mid s)$。
定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：
$$
R(\tau):=\sum_{t=0}^{T}\gamma^t r_{t+1}.
$$
目标函数（期望折扣回报）：
$$
J(\theta)=\mathbb E_{\tau\sim p_\theta}[R(\tau)].
$$
其中轨迹 $\tau$ 表示
$
\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_T,a_T,r_{T+1})
$
，轨迹分布为
$$
p_\theta(\tau)=\mu(s_0)\prod_{t=0}^{T}\pi_\theta(a_t\mid s_t),P(s_{t+1}\mid s_t,a_t).
$$

关键点：环境动力学 $P$ 与初始分布 $\mu$ 不依赖 $\theta$。$\theta$ 只通过策略 $\pi_\theta$ 进入。"><meta name=author content><link rel=canonical href=https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://analyst-huang.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://analyst-huang.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://analyst-huang.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://analyst-huang.github.io/apple-touch-icon.png><link rel=mask-icon href=https://analyst-huang.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,packages:{"[+]":["ams"]}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/"><meta property="og:site_name" content="Blog"><meta property="og:title" content="策略梯度定理"><meta property="og:description" content="策略梯度：从 REINFORCE 到策略梯度定理（含详细证明） 本文目标：
用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质； 给出 REINFORCE（Williams, 1992）的 score-function / likelihood-ratio 推导； 给出 Sutton et al.（1999）式 策略梯度定理（Policy Gradient Theorem） 的 Bellman/占用测度推导； 0. 记号与设定 我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：
状态：$s\in\mathcal S$ 动作：$a\in\mathcal A$ 转移：$P(s’\mid s,a)$ 即时奖励：$r(s,a)$（或 $r_t$） 折扣：$\gamma\in(0,1)$ 初始状态分布：$\mu(s)=\Pr(s_0=s)$ 随机策略以参数 $\theta$ 参数化：$\pi_\theta(a\mid s)$。
定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：
$$ R(\tau):=\sum_{t=0}^{T}\gamma^t r_{t+1}. $$
目标函数（期望折扣回报）：
$$ J(\theta)=\mathbb E_{\tau\sim p_\theta}[R(\tau)]. $$
其中轨迹 $\tau$ 表示 $ \tau=(s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_T,a_T,r_{T+1}) $ ，轨迹分布为
$$ p_\theta(\tau)=\mu(s_0)\prod_{t=0}^{T}\pi_\theta(a_t\mid s_t),P(s_{t+1}\mid s_t,a_t). $$
关键点：环境动力学 $P$ 与初始分布 $\mu$ 不依赖 $\theta$。$\theta$ 只通过策略 $\pi_\theta$ 进入。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-12-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="策略梯度定理"><meta name=twitter:description content="策略梯度：从 REINFORCE 到策略梯度定理（含详细证明）

本文目标：

用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质；
给出 REINFORCE（Williams, 1992）的 score-function / likelihood-ratio 推导；
给出 Sutton et al.（1999）式 策略梯度定理（Policy Gradient Theorem） 的 Bellman/占用测度推导；



0. 记号与设定
我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：

状态：$s\in\mathcal S$
动作：$a\in\mathcal A$
转移：$P(s&rsquo;\mid s,a)$
即时奖励：$r(s,a)$（或 $r_t$）
折扣：$\gamma\in(0,1)$
初始状态分布：$\mu(s)=\Pr(s_0=s)$

随机策略以参数 $\theta$ 参数化：$\pi_\theta(a\mid s)$。
定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：
$$
R(\tau):=\sum_{t=0}^{T}\gamma^t r_{t+1}.
$$
目标函数（期望折扣回报）：
$$
J(\theta)=\mathbb E_{\tau\sim p_\theta}[R(\tau)].
$$
其中轨迹 $\tau$ 表示
$
\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_T,a_T,r_{T+1})
$
，轨迹分布为
$$
p_\theta(\tau)=\mu(s_0)\prod_{t=0}^{T}\pi_\theta(a_t\mid s_t),P(s_{t+1}\mid s_t,a_t).
$$

关键点：环境动力学 $P$ 与初始分布 $\mu$ 不依赖 $\theta$。$\theta$ 只通过策略 $\pi_\theta$ 进入。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://analyst-huang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI","item":"https://analyst-huang.github.io/posts/ai/"},{"@type":"ListItem","position":3,"name":"策略梯度定理","item":"https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"策略梯度定理","name":"策略梯度定理","description":"策略梯度：从 REINFORCE 到策略梯度定理（含详细证明） 本文目标：\n用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质； 给出 REINFORCE（Williams, 1992）的 score-function / likelihood-ratio 推导； 给出 Sutton et al.（1999）式 策略梯度定理（Policy Gradient Theorem） 的 Bellman/占用测度推导； 0. 记号与设定 我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：\n状态：$s\\in\\mathcal S$ 动作：$a\\in\\mathcal A$ 转移：$P(s\u0026rsquo;\\mid s,a)$ 即时奖励：$r(s,a)$（或 $r_t$） 折扣：$\\gamma\\in(0,1)$ 初始状态分布：$\\mu(s)=\\Pr(s_0=s)$ 随机策略以参数 $\\theta$ 参数化：$\\pi_\\theta(a\\mid s)$。\n定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：\n$$ R(\\tau):=\\sum_{t=0}^{T}\\gamma^t r_{t+1}. $$\n目标函数（期望折扣回报）：\n$$ J(\\theta)=\\mathbb E_{\\tau\\sim p_\\theta}[R(\\tau)]. $$\n其中轨迹 $\\tau$ 表示 $ \\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\\ldots,s_T,a_T,r_{T+1}) $ ，轨迹分布为\n$$ p_\\theta(\\tau)=\\mu(s_0)\\prod_{t=0}^{T}\\pi_\\theta(a_t\\mid s_t),P(s_{t+1}\\mid s_t,a_t). $$\n关键点：环境动力学 $P$ 与初始分布 $\\mu$ 不依赖 $\\theta$。$\\theta$ 只通过策略 $\\pi_\\theta$ 进入。\n","keywords":[],"articleBody":"策略梯度：从 REINFORCE 到策略梯度定理（含详细证明） 本文目标：\n用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质； 给出 REINFORCE（Williams, 1992）的 score-function / likelihood-ratio 推导； 给出 Sutton et al.（1999）式 策略梯度定理（Policy Gradient Theorem） 的 Bellman/占用测度推导； 0. 记号与设定 我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：\n状态：$s\\in\\mathcal S$ 动作：$a\\in\\mathcal A$ 转移：$P(s’\\mid s,a)$ 即时奖励：$r(s,a)$（或 $r_t$） 折扣：$\\gamma\\in(0,1)$ 初始状态分布：$\\mu(s)=\\Pr(s_0=s)$ 随机策略以参数 $\\theta$ 参数化：$\\pi_\\theta(a\\mid s)$。\n定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：\n$$ R(\\tau):=\\sum_{t=0}^{T}\\gamma^t r_{t+1}. $$\n目标函数（期望折扣回报）：\n$$ J(\\theta)=\\mathbb E_{\\tau\\sim p_\\theta}[R(\\tau)]. $$\n其中轨迹 $\\tau$ 表示 $ \\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\\ldots,s_T,a_T,r_{T+1}) $ ，轨迹分布为\n$$ p_\\theta(\\tau)=\\mu(s_0)\\prod_{t=0}^{T}\\pi_\\theta(a_t\\mid s_t),P(s_{t+1}\\mid s_t,a_t). $$\n关键点：环境动力学 $P$ 与初始分布 $\\mu$ 不依赖 $\\theta$。$\\theta$ 只通过策略 $\\pi_\\theta$ 进入。\n1. 历史脉络：先有经验更新，再有结构化定理 1992：REINFORCE（Williams）\n从统计学的 score-function 恒等式出发，直接把 $\\nabla_\\theta J$ 写成轨迹期望，并用蒙特卡洛采样得到无偏估计。 这是一种“算法先行”的路径：证明的核心是概率恒等式，而不是 Bellman/占用测度。 1999：策略梯度定理（Sutton, McAllester, Singh, Mansour）\n利用 MDP 的马尔可夫结构，把梯度写成“折扣占用测度 $\\rho^\\pi$ 下的 $\\nabla\\log\\pi\\cdot Q$ 期望”。 该形式与函数逼近、actor-critic、优势估计（GAE）等现代方法天然兼容。 本文将证明：两者表达的是 同一个真实梯度，差别主要在于“对轨迹积分”还是“对状态占用积分”的坐标系不同。\n2. REINFORCE：当时的方法（score-function / likelihood-ratio）推导 2.1 基本恒等式：score-function 对任意可微分布 $p_\\theta(x)$ 与可积函数 $f(x)$：\n$$ \\nabla_\\theta\\mathbb E_{x\\sim p_\\theta}[f(x)] =\\nabla_\\theta\\int f(x)p_\\theta(x)dx =\\int f(x)\\nabla_\\theta p_\\theta(x)dx. $$\n利用恒等式 $\\nabla p = p,\\nabla\\log p$：\n$$ \\int f(x)\\nabla_\\theta p_\\theta(x)dx =\\int f(x)p_\\theta(x)\\nabla_\\theta\\log p_\\theta(x)dx =\\mathbb E_{x\\sim p_\\theta}[f(x)\\nabla_\\theta\\log p_\\theta(x)]. $$\n这就是 score-function（亦称 likelihood-ratio）梯度恒等式。\n2.2 应用到轨迹分布 令 $x=\\tau$，$f(\\tau)=R(\\tau)$，得到\n$$ \\nabla_\\theta J(\\theta) =\\mathbb E_{\\tau\\sim p_\\theta}[R(\\tau)\\nabla_\\theta\\log p_\\theta(\\tau)]. $$\n现在展开轨迹对数概率：\n$$ \\log p_\\theta(\\tau) =\\log\\mu(s_0)+\\sum_{t=0}^{T}\\bigl(\\log\\pi_\\theta(a_t\\mid s_t)+\\log P(s_{t+1}\\mid s_t,a_t)\\bigr). $$\n对 $\\theta$ 求导时，$\\mu$ 与 $P$ 不依赖 $\\theta$，因此\n$$ \\nabla_\\theta\\log p_\\theta(\\tau) =\\sum_{t=0}^{T}\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t). $$\n代回：\n$$ \\boxed{ \\nabla_\\theta J(\\theta) =\\mathbb E_{\\tau\\sim p_\\theta}\\Big[\\Big(\\sum_{t=0}^{T}\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\Big)R(\\tau)\\Big]. } $$\n2.3 “因果性（causality）”改写：用 $G_t$ 替代全回报 上式把同一条轨迹的全回报 $R(\\tau)$ 乘到每个时间步的 $\\nabla\\log\\pi$ 上，会引入无谓方差。\n注意到：在时间步 $t$，$\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)$ 只依赖 $(s_t,a_t)$，与过去奖励无关；而 $R(\\tau)=\\sum_{k=0}^{T}\\gamma^k r_{k+1}$ 可拆为“过去 + 未来”。\n定义从 $t$ 开始的后续折扣回报：\n$$ G_t:=\\sum_{k=t}^{T}\\gamma^{k-t}r_{k+1}. $$\n可证明（通过对条件期望的展开）\n$$ \\mathbb E\\big[\\nabla\\log\\pi(a_t\\mid s_t)\\cdot (\\text{past rewards})\\big]=0, $$\n因果性（简短说明） 这里的“因果性”不是哲学意义上的因果，而是时间—信息结构约束：\n在时刻 $t$，动作 $a_t$ 是在给定过去历史 $H_{t-1}$ 与当前状态 $s_t$ 之后，按策略 $\\pi_\\theta(a_t\\mid s_t)$ 新采样得到的随机变量；而 past rewards 只依赖于 $H_{t-1}$，与 $a_t$ 无关。\n因此，在只给定过去信息的条件下，对当前动作采样所产生的 $\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)$ 取期望，其均值为 $0$。这保证了：在 reward 尚未出现之前，参数更新不具有系统性偏向。\n证明 设 $X$ 为任意仅依赖过去的随机变量（例如 past rewards），$H_{t-1}$ 为到 $t-1$ 为止的历史。由全期望公式有\n$$\\mathbb E[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)X]=\\mathbb E[\\mathbb E[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)X\\mid H_{t-1}]].$$\n由于 $X$ 对 $H_{t-1}$ 可测，可提出得\n$$\\mathbb E[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)X]=\\mathbb E[X\\cdot\\mathbb E[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\mid H_{t-1}]].$$\n给定 $H_{t-1}$ 时，$s_t$ 已确定，且 $a_t\\sim\\pi_\\theta(\\cdot\\mid s_t)$，因此\n$$\\mathbb E[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\mid H_{t-1}]=\\sum_a\\pi_\\theta(a\\mid s_t)\\nabla_\\theta\\log\\pi_\\theta(a\\mid s_t)=\\sum_a\\nabla_\\theta\\pi_\\theta(a\\mid s_t)=\\nabla_\\theta 1=0.$$\n代回即得\n$$\\mathbb E[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t),X]=0.$$\n因此\n$$ \\boxed{ \\nabla_\\theta J(\\theta) =\\mathbb E_{\\tau\\sim p_\\theta}\\Big[\\sum_{t=0}^{T}\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)G_t\\Big]. } $$\n这就是最常见的 REINFORCE 梯度表达。\n2.4 REINFORCE 更新公式（蒙特卡洛估计） 用 $N$ 条独立轨迹采样估计上述期望：\n$$ \\hat g =\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=0}^{T}\\nabla_\\theta\\log\\pi_\\theta(a_{t,n}\\mid s_{t,n})G_{t,n}, \\qquad \\theta\\leftarrow\\theta+\\alpha\\hat g. $$\n重要结论：REINFORCE 估计是 无偏 的（对真实 $\\nabla J$ 的无偏估计），但通常 方差很高。\n2.5 baseline：不改变期望、降低方差 对任意仅依赖状态的 $b(s)$：\n$$ \\mathbb E_{a\\sim\\pi_\\theta(\\cdot\\mid s)}[\\nabla_\\theta\\log\\pi_\\theta(a\\mid s)b(s)] =b(s)\\sum_a \\pi_\\theta(a\\mid s)\\nabla_\\theta\\log\\pi_\\theta(a\\mid s) =b(s)\\sum_a\\nabla_\\theta\\pi_\\theta(a\\mid s) =b(s)\\nabla_\\theta 1=0. $$\n因此可用 $G_t-b(s_t)$ 替代 $G_t$ 而不改变期望。\n3. 策略梯度定理：从 Bellman 递归到占用测度 $\\rho^\\pi$ 这一部分是我们讨论中“符号最多”的地方。核心思路是：\n$\\nabla V^\\pi$ 也满足一个 Bellman 型递归； 把这个递归展开后，自然出现折扣占用测度 $\\rho^\\pi$。\n3.1 定义值函数与 Q 函数 $$ V^\\pi(s)=\\mathbb E_\\pi\\Big[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\mid s_0=s\\Big], \\qquad Q^\\pi(s,a)=\\mathbb E_\\pi\\Big[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\mid s_0=s,a_0=a\\Big]. $$\n目标函数可写为\n$$ J(\\theta)=\\mathbb E_{s_0\\sim\\mu}[V^{\\pi_\\theta}(s_0)]=\\sum_s\\mu(s)V^{\\pi_\\theta}(s). $$\n3.2 策略诱导转移核 $P_\\pi$ 定义\n$$ P_\\pi(s’\\mid s):=\\sum_a\\pi_\\theta(a\\mid s)P(s’\\mid s,a). $$\n并用算子写法 $(P_\\pi f)(s)=\\sum_{s’}P_\\pi(s’\\mid s)f(s’)$。\n3.3 关键递归：$\\nabla V^\\pi$ 的 Bellman 型方程 Bellman 期望方程：\n$$ V^\\pi(s)=\\sum_a\\pi_\\theta(a\\mid s)Q^\\pi(s,a). $$\n对 $\\theta$ 求导：\n$$ \\nabla V^\\pi(s)=\\sum_a\\nabla\\pi_\\theta(a\\mid s)Q^\\pi(s,a)+\\sum_a\\pi_\\theta(a\\mid s)\\nabla Q^\\pi(s,a). $$\n而\n$$ Q^\\pi(s,a)=r(s,a)+\\gamma\\sum_{s’}P(s’\\mid s,a)V^\\pi(s’), $$\n因此\n$$ \\nabla Q^\\pi(s,a)=\\gamma\\sum_{s’}P(s’\\mid s,a)\\nabla V^\\pi(s’). $$\n代回得\n$$\\nabla V^\\pi(s)=g_\\theta(s)+\\gamma\\sum_{s’}\\Big(\\sum_a\\pi_\\theta(a\\mid s)P(s’\\mid s,a)\\Big)\\nabla V^\\pi(s’),\\quad g_\\theta(s):=\\sum_a \\nabla\\pi_\\theta(a\\mid s),Q^\\pi(s,a).$$\n即\n$$ \\boxed{ \\nabla V^\\pi = g_\\theta + \\gamma P_\\pi(\\nabla V^\\pi). } $$\n其中\n$$ \\boxed{ g_\\theta(s)=\\sum_a\\nabla\\pi_\\theta(a\\mid s),Q^\\pi(s,a). } $$\n3.4 展开递归：Neumann 级数与“未来影响链” 将上式改写为线性方程：\n$$ (I-\\gamma P_\\pi)\\nabla V^\\pi=g_\\theta. $$\n在适当条件下（例如有限状态且 $\\gamma\u003c1$），\n$$ (I-\\gamma P_\\pi)^{-1}=\\sum_{t=0}^{\\infty}(\\gamma P_\\pi)^t. $$\n因此\n$$ \\boxed{ \\nabla V^\\pi=\\sum_{t=0}^{\\infty}\\gamma^t P_\\pi^t g_\\theta. } $$\n该式表达：$\\nabla V$ 等于“立即贡献 $g$”加上“传到下一步、下下步……的累计影响”。\n3.5 从 $\\nabla V$ 推到 $\\nabla J$：折扣占用测度出现 $$ \\nabla J(\\theta)=\\sum_s\\mu(s)\\nabla V^\\pi(s) =\\sum_s\\mu(s)\\sum_{t=0}^{\\infty}\\gamma^t (P_\\pi^t g_\\theta)(s). $$\n注意 $\\mu P_\\pi^t$ 给出在策略 $\\pi$ 下从初始分布出发走 $t$ 步的状态边际分布。于是可写为\n$$ \\nabla J(\\theta)=\\sum_{t=0}^{\\infty}\\gamma^t\\sum_s\\Pr_\\pi(s_t=s)g_\\theta(s). $$\n定义（未归一化的）折扣状态占用测度：\n$$ \\boxed{ \\rho^\\pi(s):=\\sum_{t=0}^{\\infty}\\gamma^t\\Pr_\\pi(s_t=s). } $$\n于是\n$$ \\nabla J(\\theta)=\\sum_s\\rho^\\pi(s)g_\\theta(s) =\\sum_s\\rho^\\pi(s)\\sum_a\\nabla\\pi_\\theta(a\\mid s)Q^\\pi(s,a). $$\n最后用 $\\nabla\\pi=\\pi\\nabla\\log\\pi$ 得\n$$ \\boxed{ \\nabla_\\theta J(\\theta) =\\mathbb E_{s\\sim\\rho^\\pi,\\ a\\sim\\pi_\\theta(\\cdot\\mid s)}\\big[\\nabla_\\theta\\log\\pi_\\theta(a\\mid s)Q^\\pi(s,a)\\big]. } $$\n这就是策略梯度定理。\n","wordCount":"560","inLanguage":"en","datePublished":"2025-12-30T00:00:00Z","dateModified":"2025-12-30T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://analyst-huang.github.io/posts/ai/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/"},"publisher":{"@type":"Organization","name":"Blog","logo":{"@type":"ImageObject","url":"https://analyst-huang.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://analyst-huang.github.io/ accesskey=h title="Blog (Alt + H)">Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://analyst-huang.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://analyst-huang.github.io/about/ title=关于><span>关于</span></a></li><li><a href=https://analyst-huang.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://analyst-huang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://analyst-huang.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://analyst-huang.github.io/posts/ai/>AI</a></div><h1 class="post-title entry-hint-parent">策略梯度定理</h1><div class=post-meta><span title='2025-12-30 00:00:00 +0000 UTC'>December 30, 2025</span>&nbsp;·&nbsp;<span>3 min</span></div></header><div class=post-content><h1 id=策略梯度从-reinforce-到策略梯度定理含详细证明>策略梯度：从 REINFORCE 到策略梯度定理（含详细证明）<a hidden class=anchor aria-hidden=true href=#策略梯度从-reinforce-到策略梯度定理含详细证明>#</a></h1><blockquote><p>本文目标：</p><ol><li>用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质；</li><li>给出 REINFORCE（Williams, 1992）的 <strong>score-function / likelihood-ratio</strong> 推导；</li><li>给出 Sutton et al.（1999）式 <strong>策略梯度定理（Policy Gradient Theorem）</strong> 的 Bellman/占用测度推导；</li></ol></blockquote><hr><h2 id=0-记号与设定>0. 记号与设定<a hidden class=anchor aria-hidden=true href=#0-记号与设定>#</a></h2><p>我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：</p><ul><li>状态：$s\in\mathcal S$</li><li>动作：$a\in\mathcal A$</li><li>转移：$P(s&rsquo;\mid s,a)$</li><li>即时奖励：$r(s,a)$（或 $r_t$）</li><li>折扣：$\gamma\in(0,1)$</li><li>初始状态分布：$\mu(s)=\Pr(s_0=s)$</li></ul><p>随机策略以参数 $\theta$ 参数化：$\pi_\theta(a\mid s)$。</p><p>定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：</p><p>$$
R(\tau):=\sum_{t=0}^{T}\gamma^t r_{t+1}.
$$</p><p>目标函数（期望折扣回报）：</p><p>$$
J(\theta)=\mathbb E_{\tau\sim p_\theta}[R(\tau)].
$$</p><p>其中轨迹 $\tau$ 表示
$
\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_T,a_T,r_{T+1})
$
，轨迹分布为</p><p>$$
p_\theta(\tau)=\mu(s_0)\prod_{t=0}^{T}\pi_\theta(a_t\mid s_t),P(s_{t+1}\mid s_t,a_t).
$$</p><blockquote><p>关键点：环境动力学 $P$ 与初始分布 $\mu$ 不依赖 $\theta$。$\theta$ 只通过策略 $\pi_\theta$ 进入。</p></blockquote><hr><h2 id=1-历史脉络先有经验更新再有结构化定理>1. 历史脉络：先有经验更新，再有结构化定理<a hidden class=anchor aria-hidden=true href=#1-历史脉络先有经验更新再有结构化定理>#</a></h2><ul><li><p><strong>1992：REINFORCE（Williams）</strong></p><ul><li>从统计学的 score-function 恒等式出发，直接把 $\nabla_\theta J$ 写成轨迹期望，并用蒙特卡洛采样得到无偏估计。</li><li>这是一种“算法先行”的路径：证明的核心是概率恒等式，而不是 Bellman/占用测度。</li></ul></li><li><p><strong>1999：策略梯度定理（Sutton, McAllester, Singh, Mansour）</strong></p><ul><li>利用 MDP 的马尔可夫结构，把梯度写成“折扣占用测度 $\rho^\pi$ 下的 $\nabla\log\pi\cdot Q$ 期望”。</li><li>该形式与函数逼近、actor-critic、优势估计（GAE）等现代方法天然兼容。</li></ul></li></ul><p>本文将证明：两者表达的是 <strong>同一个真实梯度</strong>，差别主要在于“对轨迹积分”还是“对状态占用积分”的坐标系不同。</p><hr><h2 id=2-reinforce当时的方法score-function--likelihood-ratio推导>2. REINFORCE：当时的方法（score-function / likelihood-ratio）推导<a hidden class=anchor aria-hidden=true href=#2-reinforce当时的方法score-function--likelihood-ratio推导>#</a></h2><h3 id=21-基本恒等式score-function>2.1 基本恒等式：score-function<a hidden class=anchor aria-hidden=true href=#21-基本恒等式score-function>#</a></h3><p>对任意可微分布 $p_\theta(x)$ 与可积函数 $f(x)$：</p><p>$$
\nabla_\theta\mathbb E_{x\sim p_\theta}[f(x)]
=\nabla_\theta\int f(x)p_\theta(x)dx
=\int f(x)\nabla_\theta p_\theta(x)dx.
$$</p><p>利用恒等式 $\nabla p = p,\nabla\log p$：</p><p>$$
\int f(x)\nabla_\theta p_\theta(x)dx
=\int f(x)p_\theta(x)\nabla_\theta\log p_\theta(x)dx
=\mathbb E_{x\sim p_\theta}[f(x)\nabla_\theta\log p_\theta(x)].
$$</p><p>这就是 score-function（亦称 likelihood-ratio）梯度恒等式。</p><h3 id=22-应用到轨迹分布>2.2 应用到轨迹分布<a hidden class=anchor aria-hidden=true href=#22-应用到轨迹分布>#</a></h3><p>令 $x=\tau$，$f(\tau)=R(\tau)$，得到</p><p>$$
\nabla_\theta J(\theta)
=\mathbb E_{\tau\sim p_\theta}[R(\tau)\nabla_\theta\log p_\theta(\tau)].
$$</p><p>现在展开轨迹对数概率：</p><p>$$
\log p_\theta(\tau)
=\log\mu(s_0)+\sum_{t=0}^{T}\bigl(\log\pi_\theta(a_t\mid s_t)+\log P(s_{t+1}\mid s_t,a_t)\bigr).
$$</p><p>对 $\theta$ 求导时，$\mu$ 与 $P$ 不依赖 $\theta$，因此</p><p>$$
\nabla_\theta\log p_\theta(\tau)
=\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t).
$$</p><p>代回：</p><p>$$
\boxed{
\nabla_\theta J(\theta)
=\mathbb E_{\tau\sim p_\theta}\Big[\Big(\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)\Big)R(\tau)\Big].
}
$$</p><h3 id=23-因果性causality改写用-g_t-替代全回报>2.3 “因果性（causality）”改写：用 $G_t$ 替代全回报<a hidden class=anchor aria-hidden=true href=#23-因果性causality改写用-g_t-替代全回报>#</a></h3><p>上式把同一条轨迹的全回报 $R(\tau)$ 乘到每个时间步的 $\nabla\log\pi$ 上，会引入无谓方差。</p><p>注意到：在时间步 $t$，$\nabla_\theta\log\pi_\theta(a_t\mid s_t)$ 只依赖 $(s_t,a_t)$，与过去奖励无关；而 $R(\tau)=\sum_{k=0}^{T}\gamma^k r_{k+1}$ 可拆为“过去 + 未来”。</p><p>定义从 $t$ 开始的后续折扣回报：</p><p>$$
G_t:=\sum_{k=t}^{T}\gamma^{k-t}r_{k+1}.
$$</p><p>可证明（通过对条件期望的展开）</p><p>$$
\mathbb E\big[\nabla\log\pi(a_t\mid s_t)\cdot (\text{past rewards})\big]=0,
$$</p><details><summary><strong>因果性（简短说明）</strong></summary><p>这里的“因果性”不是哲学意义上的因果，而是<strong>时间—信息结构约束</strong>：</p><p>在时刻 $t$，动作 $a_t$ 是在给定过去历史 $H_{t-1}$ 与当前状态 $s_t$ 之后，按策略 $\pi_\theta(a_t\mid s_t)$ 新采样得到的随机变量；而 past rewards 只依赖于 $H_{t-1}$，与 $a_t$ 无关。</p><p>因此，在只给定过去信息的条件下，对当前动作采样所产生的 $\nabla_\theta \log \pi_\theta(a_t\mid s_t)$ 取期望，其均值为 $0$。这保证了：<strong>在 reward 尚未出现之前，参数更新不具有系统性偏向。</strong></p></details><details><summary><strong>证明</strong></summary><p>设 $X$ 为任意仅依赖过去的随机变量（例如 past rewards），$H_{t-1}$ 为到 $t-1$ 为止的历史。由全期望公式有<br>$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)X]=\mathbb E[\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)X\mid H_{t-1}]].$$</p><p>由于 $X$ 对 $H_{t-1}$ 可测，可提出得<br>$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)X]=\mathbb E[X\cdot\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\mid H_{t-1}]].$$</p><p>给定 $H_{t-1}$ 时，$s_t$ 已确定，且 $a_t\sim\pi_\theta(\cdot\mid s_t)$，因此<br>$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\mid H_{t-1}]=\sum_a\pi_\theta(a\mid s_t)\nabla_\theta\log\pi_\theta(a\mid s_t)=\sum_a\nabla_\theta\pi_\theta(a\mid s_t)=\nabla_\theta 1=0.$$</p><p>代回即得<br>$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t),X]=0.$$</p></details><p></p><p>因此</p><p>$$
\boxed{
\nabla_\theta J(\theta)
=\mathbb E_{\tau\sim p_\theta}\Big[\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)G_t\Big].
}
$$</p><p>这就是最常见的 REINFORCE 梯度表达。</p><h3 id=24-reinforce-更新公式蒙特卡洛估计>2.4 REINFORCE 更新公式（蒙特卡洛估计）<a hidden class=anchor aria-hidden=true href=#24-reinforce-更新公式蒙特卡洛估计>#</a></h3><p>用 $N$ 条独立轨迹采样估计上述期望：</p><p>$$
\hat g
=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_{t,n}\mid s_{t,n})G_{t,n},
\qquad
\theta\leftarrow\theta+\alpha\hat g.
$$</p><blockquote><p>重要结论：REINFORCE 估计是 <strong>无偏</strong> 的（对真实 $\nabla J$ 的无偏估计），但通常 <strong>方差很高</strong>。</p></blockquote><h3 id=25-baseline不改变期望降低方差>2.5 baseline：不改变期望、降低方差<a hidden class=anchor aria-hidden=true href=#25-baseline不改变期望降低方差>#</a></h3><p>对任意仅依赖状态的 $b(s)$：</p><p>$$
\mathbb E_{a\sim\pi_\theta(\cdot\mid s)}[\nabla_\theta\log\pi_\theta(a\mid s)b(s)]
=b(s)\sum_a \pi_\theta(a\mid s)\nabla_\theta\log\pi_\theta(a\mid s)
=b(s)\sum_a\nabla_\theta\pi_\theta(a\mid s)
=b(s)\nabla_\theta 1=0.
$$</p><p>因此可用 $G_t-b(s_t)$ 替代 $G_t$ 而不改变期望。</p><hr><h2 id=3-策略梯度定理从-bellman-递归到占用测度-rhopi>3. 策略梯度定理：从 Bellman 递归到占用测度 $\rho^\pi$<a hidden class=anchor aria-hidden=true href=#3-策略梯度定理从-bellman-递归到占用测度-rhopi>#</a></h2><p>这一部分是我们讨论中“符号最多”的地方。核心思路是：</p><blockquote><p>$\nabla V^\pi$ 也满足一个 Bellman 型递归；
把这个递归展开后，自然出现折扣占用测度 $\rho^\pi$。</p></blockquote><h3 id=31-定义值函数与-q-函数>3.1 定义值函数与 Q 函数<a hidden class=anchor aria-hidden=true href=#31-定义值函数与-q-函数>#</a></h3><p>$$
V^\pi(s)=\mathbb E_\pi\Big[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\mid s_0=s\Big],
\qquad
Q^\pi(s,a)=\mathbb E_\pi\Big[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\mid s_0=s,a_0=a\Big].
$$</p><p>目标函数可写为</p><p>$$
J(\theta)=\mathbb E_{s_0\sim\mu}[V^{\pi_\theta}(s_0)]=\sum_s\mu(s)V^{\pi_\theta}(s).
$$</p><h3 id=32-策略诱导转移核-p_pi>3.2 策略诱导转移核 $P_\pi$<a hidden class=anchor aria-hidden=true href=#32-策略诱导转移核-p_pi>#</a></h3><p>定义</p><p>$$
P_\pi(s&rsquo;\mid s):=\sum_a\pi_\theta(a\mid s)P(s&rsquo;\mid s,a).
$$</p><p>并用算子写法 $(P_\pi f)(s)=\sum_{s&rsquo;}P_\pi(s&rsquo;\mid s)f(s&rsquo;)$。</p><h3 id=33-关键递归nabla-vpi-的-bellman-型方程>3.3 关键递归：$\nabla V^\pi$ 的 Bellman 型方程<a hidden class=anchor aria-hidden=true href=#33-关键递归nabla-vpi-的-bellman-型方程>#</a></h3><p>Bellman 期望方程：</p><p>$$
V^\pi(s)=\sum_a\pi_\theta(a\mid s)Q^\pi(s,a).
$$</p><p>对 $\theta$ 求导：</p><p>$$
\nabla V^\pi(s)=\sum_a\nabla\pi_\theta(a\mid s)Q^\pi(s,a)+\sum_a\pi_\theta(a\mid s)\nabla Q^\pi(s,a).
$$</p><p>而</p><p>$$
Q^\pi(s,a)=r(s,a)+\gamma\sum_{s&rsquo;}P(s&rsquo;\mid s,a)V^\pi(s&rsquo;),
$$</p><p>因此</p><p>$$
\nabla Q^\pi(s,a)=\gamma\sum_{s&rsquo;}P(s&rsquo;\mid s,a)\nabla V^\pi(s&rsquo;).
$$</p><p>代回得</p><p>$$\nabla V^\pi(s)=g_\theta(s)+\gamma\sum_{s&rsquo;}\Big(\sum_a\pi_\theta(a\mid s)P(s&rsquo;\mid s,a)\Big)\nabla V^\pi(s&rsquo;),\quad g_\theta(s):=\sum_a \nabla\pi_\theta(a\mid s),Q^\pi(s,a).$$</p><p>即</p><p>$$
\boxed{
\nabla V^\pi = g_\theta + \gamma P_\pi(\nabla V^\pi).
}
$$</p><p>其中</p><p>$$
\boxed{
g_\theta(s)=\sum_a\nabla\pi_\theta(a\mid s),Q^\pi(s,a).
}
$$</p><h3 id=34-展开递归neumann-级数与未来影响链>3.4 展开递归：Neumann 级数与“未来影响链”<a hidden class=anchor aria-hidden=true href=#34-展开递归neumann-级数与未来影响链>#</a></h3><p>将上式改写为线性方程：</p><p>$$
(I-\gamma P_\pi)\nabla V^\pi=g_\theta.
$$</p><p>在适当条件下（例如有限状态且 $\gamma&lt;1$），</p><p>$$
(I-\gamma P_\pi)^{-1}=\sum_{t=0}^{\infty}(\gamma P_\pi)^t.
$$</p><p>因此</p><p>$$
\boxed{
\nabla V^\pi=\sum_{t=0}^{\infty}\gamma^t P_\pi^t g_\theta.
}
$$</p><p>该式表达：$\nabla V$ 等于“立即贡献 $g$”加上“传到下一步、下下步……的累计影响”。</p><h3 id=35-从-nabla-v-推到-nabla-j折扣占用测度出现>3.5 从 $\nabla V$ 推到 $\nabla J$：折扣占用测度出现<a hidden class=anchor aria-hidden=true href=#35-从-nabla-v-推到-nabla-j折扣占用测度出现>#</a></h3><p>$$
\nabla J(\theta)=\sum_s\mu(s)\nabla V^\pi(s)
=\sum_s\mu(s)\sum_{t=0}^{\infty}\gamma^t (P_\pi^t g_\theta)(s).
$$</p><p>注意 $\mu P_\pi^t$ 给出在策略 $\pi$ 下从初始分布出发走 $t$ 步的状态边际分布。于是可写为</p><p>$$
\nabla J(\theta)=\sum_{t=0}^{\infty}\gamma^t\sum_s\Pr_\pi(s_t=s)g_\theta(s).
$$</p><p>定义（未归一化的）折扣状态占用测度：</p><p>$$
\boxed{
\rho^\pi(s):=\sum_{t=0}^{\infty}\gamma^t\Pr_\pi(s_t=s).
}
$$</p><p>于是</p><p>$$
\nabla J(\theta)=\sum_s\rho^\pi(s)g_\theta(s)
=\sum_s\rho^\pi(s)\sum_a\nabla\pi_\theta(a\mid s)Q^\pi(s,a).
$$</p><p>最后用 $\nabla\pi=\pi\nabla\log\pi$ 得</p><p>$$
\boxed{
\nabla_\theta J(\theta)
=\mathbb E_{s\sim\rho^\pi,\ a\sim\pi_\theta(\cdot\mid s)}\big[\nabla_\theta\log\pi_\theta(a\mid s)Q^\pi(s,a)\big].
}
$$</p><p>这就是策略梯度定理。</p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://analyst-huang.github.io/>Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>