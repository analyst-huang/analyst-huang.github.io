<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>World Model for Dexterous Manipulation | Blog</title><meta name=keywords content><meta name=description content="Model Predictive Control(MPC)
Deep Dynamics Models for Learning Dexterous Manipulation
TLDR
Online Collect data and learn, just like PPO, but learn dynamic model using surpervised learning instead of using reward signal to optimize a policy.
Limitation
not good for long horizon, dense reward preferred
Utility
since the paradigm of demo replay is always short horizon, so a dynamic model may be easily trained?

how to solve the object geometry problem

do not use image, image can not support planning
we can use the together learned RL policy to avoid blind sampling at the first stage
but one env need sample many traj for planning
how to integerate object geometry? a universal encoder?
Most importantly, no evidence or justified belief that it will work better than PPO. It may work just as good as PPO at the same training  cost, or even worse.
but still exhibit a high coherence with demo replay paradigm!
maybe better, as the interplotation of the state transition is similar to the edtiion mechanism.
worth tring, it not work.
by combine them together, may help each other learn.



Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach
TLDR
High level MPC(differentiable), add tactile compensation. Online Planning."><meta name=author content><link rel=canonical href=https://analyst-huang.github.io/posts/%E6%9D%82%E8%B0%88/world_model_for_dexterous_manipulation/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://analyst-huang.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://analyst-huang.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://analyst-huang.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://analyst-huang.github.io/apple-touch-icon.png><link rel=mask-icon href=https://analyst-huang.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://analyst-huang.github.io/posts/%E6%9D%82%E8%B0%88/world_model_for_dexterous_manipulation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\\\(",right:"\\\\)",display:!1},{left:"\\\\[",right:"\\\\]",display:!0}]})})</script><meta property="og:url" content="https://analyst-huang.github.io/posts/%E6%9D%82%E8%B0%88/world_model_for_dexterous_manipulation/"><meta property="og:site_name" content="Blog"><meta property="og:title" content="World Model for Dexterous Manipulation"><meta property="og:description" content="Model Predictive Control(MPC) Deep Dynamics Models for Learning Dexterous Manipulation TLDR Online Collect data and learn, just like PPO, but learn dynamic model using surpervised learning instead of using reward signal to optimize a policy.
Limitation not good for long horizon, dense reward preferred
Utility since the paradigm of demo replay is always short horizon, so a dynamic model may be easily trained?
how to solve the object geometry problem do not use image, image can not support planning we can use the together learned RL policy to avoid blind sampling at the first stage but one env need sample many traj for planning how to integerate object geometry? a universal encoder? Most importantly, no evidence or justified belief that it will work better than PPO. It may work just as good as PPO at the same training cost, or even worse. but still exhibit a high coherence with demo replay paradigm! maybe better, as the interplotation of the state transition is similar to the edtiion mechanism. worth tring, it not work. by combine them together, may help each other learn. Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach TLDR High level MPC(differentiable), add tactile compensation. Online Planning."><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-02T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="World Model for Dexterous Manipulation"><meta name=twitter:description content="Model Predictive Control(MPC)
Deep Dynamics Models for Learning Dexterous Manipulation
TLDR
Online Collect data and learn, just like PPO, but learn dynamic model using surpervised learning instead of using reward signal to optimize a policy.
Limitation
not good for long horizon, dense reward preferred
Utility
since the paradigm of demo replay is always short horizon, so a dynamic model may be easily trained?

how to solve the object geometry problem

do not use image, image can not support planning
we can use the together learned RL policy to avoid blind sampling at the first stage
but one env need sample many traj for planning
how to integerate object geometry? a universal encoder?
Most importantly, no evidence or justified belief that it will work better than PPO. It may work just as good as PPO at the same training  cost, or even worse.
but still exhibit a high coherence with demo replay paradigm!
maybe better, as the interplotation of the state transition is similar to the edtiion mechanism.
worth tring, it not work.
by combine them together, may help each other learn.



Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach
TLDR
High level MPC(differentiable), add tactile compensation. Online Planning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://analyst-huang.github.io/posts/"},{"@type":"ListItem","position":3,"name":"World Model for Dexterous Manipulation","item":"https://analyst-huang.github.io/posts/%E6%9D%82%E8%B0%88/world_model_for_dexterous_manipulation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"World Model for Dexterous Manipulation","name":"World Model for Dexterous Manipulation","description":"Model Predictive Control(MPC) Deep Dynamics Models for Learning Dexterous Manipulation TLDR Online Collect data and learn, just like PPO, but learn dynamic model using surpervised learning instead of using reward signal to optimize a policy.\nLimitation not good for long horizon, dense reward preferred\nUtility since the paradigm of demo replay is always short horizon, so a dynamic model may be easily trained?\nhow to solve the object geometry problem do not use image, image can not support planning we can use the together learned RL policy to avoid blind sampling at the first stage but one env need sample many traj for planning how to integerate object geometry? a universal encoder? Most importantly, no evidence or justified belief that it will work better than PPO. It may work just as good as PPO at the same training cost, or even worse. but still exhibit a high coherence with demo replay paradigm! maybe better, as the interplotation of the state transition is similar to the edtiion mechanism. worth tring, it not work. by combine them together, may help each other learn. Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach TLDR High level MPC(differentiable), add tactile compensation. Online Planning.\n","keywords":[],"articleBody":"Model Predictive Control(MPC) Deep Dynamics Models for Learning Dexterous Manipulation TLDR Online Collect data and learn, just like PPO, but learn dynamic model using surpervised learning instead of using reward signal to optimize a policy.\nLimitation not good for long horizon, dense reward preferred\nUtility since the paradigm of demo replay is always short horizon, so a dynamic model may be easily trained?\nhow to solve the object geometry problem do not use image, image can not support planning we can use the together learned RL policy to avoid blind sampling at the first stage but one env need sample many traj for planning how to integerate object geometry? a universal encoder? Most importantly, no evidence or justified belief that it will work better than PPO. It may work just as good as PPO at the same training cost, or even worse. but still exhibit a high coherence with demo replay paradigm! maybe better, as the interplotation of the state transition is similar to the edtiion mechanism. worth tring, it not work. by combine them together, may help each other learn. Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach TLDR High level MPC(differentiable), add tactile compensation. Online Planning.\nLimitation Never can conduct real world experiment\nUtility baseline? guidance in simulator? Not that good, it need too much previlieged information and is somehow time-consuming and unable to batchfied.\nWorld Model DreamerV2: MASTERING ATARI WITH DISCRETE WORLD MODELS TLDR Use a sequential VAE as a world model, and train RL in latent space\nLimitation Atari的环境比较简单，对于更复杂的环境、物理、图像，可能预测的没有那么准\nUtility 如果可以给demoreplay学到一个非常好的world model，比如说在隐空间里，那完全省去了中间的时间，这会很重要\nDayDreamer: World Models for Physical Robot Learning TLDR learning a world model to enable real world learning\n","wordCount":"278","inLanguage":"en","datePublished":"2026-01-02T00:00:00Z","dateModified":"2026-01-02T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://analyst-huang.github.io/posts/%E6%9D%82%E8%B0%88/world_model_for_dexterous_manipulation/"},"publisher":{"@type":"Organization","name":"Blog","logo":{"@type":"ImageObject","url":"https://analyst-huang.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://analyst-huang.github.io/ accesskey=h title="Blog (Alt + H)">Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://analyst-huang.github.io/posts/ title=文章><span>文章</span></a></li><li><a href=https://analyst-huang.github.io/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://analyst-huang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://analyst-huang.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">World Model for Dexterous Manipulation</h1><div class=post-meta><span title='2026-01-02 00:00:00 +0000 UTC'>January 2, 2026</span>&nbsp;·&nbsp;<span>2 min</span></div></header><div class=post-content><h1 id=model-predictive-controlmpc>Model Predictive Control(MPC)<a hidden class=anchor aria-hidden=true href=#model-predictive-controlmpc>#</a></h1><h2 id=deep-dynamics-models-for-learning-dexterous-manipulation>Deep Dynamics Models for Learning Dexterous Manipulation<a hidden class=anchor aria-hidden=true href=#deep-dynamics-models-for-learning-dexterous-manipulation>#</a></h2><h3 id=tldr>TLDR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h3><p>Online Collect data and learn, just like PPO, but learn dynamic model using surpervised learning instead of using reward signal to optimize a policy.</p><h3 id=limitation>Limitation<a hidden class=anchor aria-hidden=true href=#limitation>#</a></h3><p>not good for long horizon, dense reward preferred</p><h3 id=utility>Utility<a hidden class=anchor aria-hidden=true href=#utility>#</a></h3><p>since the paradigm of demo replay is always short horizon, so a dynamic model may be easily trained?</p><ul><li>how to solve the object geometry problem<ul><li>do not use image, image can not support planning</li><li>we can use the together learned RL policy to avoid blind sampling at the first stage</li><li>but one env need sample many traj for planning</li><li>how to integerate object geometry? a universal encoder?</li><li>Most importantly, no evidence or justified belief that it will work better than PPO. It may work just as good as PPO at the same training cost, or even worse.</li><li>but still exhibit a high coherence with demo replay paradigm!</li><li>maybe better, as the interplotation of the state transition is similar to the edtiion mechanism.</li><li>worth tring, it not work.</li><li>by combine them together, may help each other learn.</li></ul></li></ul><h2 id=contact-implicit-model-predictive-control-for-dexterous-in-hand-manipulation-a-long-horizon-and-robust-approach>Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach<a hidden class=anchor aria-hidden=true href=#contact-implicit-model-predictive-control-for-dexterous-in-hand-manipulation-a-long-horizon-and-robust-approach>#</a></h2><h3 id=tldr-1>TLDR<a hidden class=anchor aria-hidden=true href=#tldr-1>#</a></h3><p>High level MPC(differentiable), add tactile compensation. Online Planning.</p><h3 id=limitation-1>Limitation<a hidden class=anchor aria-hidden=true href=#limitation-1>#</a></h3><p>Never can conduct real world experiment</p><h3 id=utility-1>Utility<a hidden class=anchor aria-hidden=true href=#utility-1>#</a></h3><p>baseline? guidance in simulator? Not that good, it need too much previlieged information and is somehow time-consuming and unable to batchfied.</p><h1 id=world-model>World Model<a hidden class=anchor aria-hidden=true href=#world-model>#</a></h1><h2 id=dreamerv2-mastering-atari-with-discrete-world-models>DreamerV2: MASTERING ATARI WITH DISCRETE WORLD MODELS<a hidden class=anchor aria-hidden=true href=#dreamerv2-mastering-atari-with-discrete-world-models>#</a></h2><h3 id=tldr-2>TLDR<a hidden class=anchor aria-hidden=true href=#tldr-2>#</a></h3><p>Use a sequential VAE as a world model, and train RL in latent space</p><h3 id=limitation-2>Limitation<a hidden class=anchor aria-hidden=true href=#limitation-2>#</a></h3><p>Atari的环境比较简单，对于更复杂的环境、物理、图像，可能预测的没有那么准</p><h3 id=utility-2>Utility<a hidden class=anchor aria-hidden=true href=#utility-2>#</a></h3><p>如果可以给demoreplay学到一个非常好的world model，比如说在隐空间里，那完全省去了中间的时间，这会很重要</p><h2 id=daydreamer-world-models-for-physical-robot-learning>DayDreamer: World Models for Physical Robot Learning<a hidden class=anchor aria-hidden=true href=#daydreamer-world-models-for-physical-robot-learning>#</a></h2><h3 id=tldr-3>TLDR<a hidden class=anchor aria-hidden=true href=#tldr-3>#</a></h3><p>learning a world model to enable real world learning</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://analyst-huang.github.io/>Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>