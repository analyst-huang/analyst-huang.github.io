---
title: "策略梯度定理"
date: 2025-12-30
draft: false
math: true
---

# 策略梯度：从 REINFORCE 到策略梯度定理（含详细证明）

> 本文目标：
>
> 1. 用统一符号说明策略梯度家族的“同一梯度、两种坐标系”本质；
> 2. 给出 REINFORCE（Williams, 1992）的 **score-function / likelihood-ratio** 推导；
> 3. 给出 Sutton et al.（1999）式 **策略梯度定理（Policy Gradient Theorem）** 的 Bellman/占用测度推导；

---

## 0. 记号与设定

我们考虑离散时间、可数（或有限）状态动作的折扣 MDP：

* 状态：$s\in\mathcal S$
* 动作：$a\in\mathcal A$
* 转移：$P(s'\mid s,a)$
* 即时奖励：$r(s,a)$（或 $r_t$）
* 折扣：$\gamma\in(0,1)$
* 初始状态分布：$\mu(s)=\Pr(s_0=s)$

随机策略以参数 $\theta$ 参数化：$\pi_\theta(a\mid s)$。

定义折扣回报（episode 有限长度 $T$ 或无穷长度均可；下文为便于书写采用有限 $T$，无穷时取极限）：

$$
R(\tau):=\sum_{t=0}^{T}\gamma^t r_{t+1}.
$$

目标函数（期望折扣回报）：

$$
J(\theta)=\mathbb E_{\tau\sim p_\theta}[R(\tau)].
$$

其中轨迹 $\tau$ 表示
$
\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_T,a_T,r_{T+1})
$
，轨迹分布为

$$
p_\theta(\tau)=\mu(s_0)\prod_{t=0}^{T}\pi_\theta(a_t\mid s_t),P(s_{t+1}\mid s_t,a_t).
$$

> 关键点：环境动力学 $P$ 与初始分布 $\mu$ 不依赖 $\theta$。$\theta$ 只通过策略 $\pi_\theta$ 进入。

---

## 1. 历史脉络：先有经验更新，再有结构化定理

* **1992：REINFORCE（Williams）**

  * 从统计学的 score-function 恒等式出发，直接把 $\nabla_\theta J$ 写成轨迹期望，并用蒙特卡洛采样得到无偏估计。
  * 这是一种“算法先行”的路径：证明的核心是概率恒等式，而不是 Bellman/占用测度。

* **1999：策略梯度定理（Sutton, McAllester, Singh, Mansour）**

  * 利用 MDP 的马尔可夫结构，把梯度写成“折扣占用测度 $\rho^\pi$ 下的 $\nabla\log\pi\cdot Q$ 期望”。
  * 该形式与函数逼近、actor-critic、优势估计（GAE）等现代方法天然兼容。

本文将证明：两者表达的是 **同一个真实梯度**，差别主要在于“对轨迹积分”还是“对状态占用积分”的坐标系不同。

---

## 2. REINFORCE：当时的方法（score-function / likelihood-ratio）推导

### 2.1 基本恒等式：score-function

对任意可微分布 $p_\theta(x)$ 与可积函数 $f(x)$：

$$
\nabla_\theta\mathbb E_{x\sim p_\theta}[f(x)]
=\nabla_\theta\int f(x)p_\theta(x)dx
=\int f(x)\nabla_\theta p_\theta(x)dx.
$$

利用恒等式 $\nabla p = p,\nabla\log p$：

$$
\int f(x)\nabla_\theta p_\theta(x)dx
=\int f(x)p_\theta(x)\nabla_\theta\log p_\theta(x)dx
=\mathbb E_{x\sim p_\theta}[f(x)\nabla_\theta\log p_\theta(x)].
$$

这就是 score-function（亦称 likelihood-ratio）梯度恒等式。

### 2.2 应用到轨迹分布

令 $x=\tau$，$f(\tau)=R(\tau)$，得到

$$
\nabla_\theta J(\theta)
=\mathbb E_{\tau\sim p_\theta}[R(\tau)\nabla_\theta\log p_\theta(\tau)].
$$

现在展开轨迹对数概率：

$$
\log p_\theta(\tau)
=\log\mu(s_0)+\sum_{t=0}^{T}\bigl(\log\pi_\theta(a_t\mid s_t)+\log P(s_{t+1}\mid s_t,a_t)\bigr).
$$

对 $\theta$ 求导时，$\mu$ 与 $P$ 不依赖 $\theta$，因此

$$
\nabla_\theta\log p_\theta(\tau)
=\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t).
$$

代回：

$$
\boxed{
\nabla_\theta J(\theta)
=\mathbb E_{\tau\sim p_\theta}\Big[\Big(\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)\Big)R(\tau)\Big].
}
$$

### 2.3 “因果性（causality）”改写：用 $G_t$ 替代全回报

上式把同一条轨迹的全回报 $R(\tau)$ 乘到每个时间步的 $\nabla\log\pi$ 上，会引入无谓方差。

注意到：在时间步 $t$，$\nabla_\theta\log\pi_\theta(a_t\mid s_t)$ 只依赖 $(s_t,a_t)$，与过去奖励无关；而 $R(\tau)=\sum_{k=0}^{T}\gamma^k r_{k+1}$ 可拆为“过去 + 未来”。

定义从 $t$ 开始的后续折扣回报：

$$
G_t:=\sum_{k=t}^{T}\gamma^{k-t}r_{k+1}.
$$

可证明（通过对条件期望的展开）

$$
\mathbb E\big[\nabla\log\pi(a_t\mid s_t)\cdot (\text{past rewards})\big]=0,
$$


<details>
<summary><strong>因果性（简短说明）</strong></summary>

这里的“因果性”不是哲学意义上的因果，而是**时间—信息结构约束**：

在时刻 $t$，动作 $a_t$ 是在给定过去历史 $H_{t-1}$ 与当前状态 $s_t$ 之后，按策略 $\pi_\theta(a_t\mid s_t)$ 新采样得到的随机变量；而 past rewards 只依赖于 $H_{t-1}$，与 $a_t$ 无关。

因此，在只给定过去信息的条件下，对当前动作采样所产生的 $\nabla_\theta \log \pi_\theta(a_t\mid s_t)$ 取期望，其均值为 $0$。这保证了：**在 reward 尚未出现之前，参数更新不具有系统性偏向。**

</details>

<details>
<summary><strong>证明</strong></summary>

设 $X$ 为任意仅依赖过去的随机变量（例如 past rewards），$H_{t-1}$ 为到 $t-1$ 为止的历史。由全期望公式有  
$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)X]=\mathbb E[\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)X\mid H_{t-1}]].$$

由于 $X$ 对 $H_{t-1}$ 可测，可提出得  
$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)X]=\mathbb E[X\cdot\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\mid H_{t-1}]].$$

给定 $H_{t-1}$ 时，$s_t$ 已确定，且 $a_t\sim\pi_\theta(\cdot\mid s_t)$，因此  
$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\mid H_{t-1}]=\sum_a\pi_\theta(a\mid s_t)\nabla_\theta\log\pi_\theta(a\mid s_t)=\sum_a\nabla_\theta\pi_\theta(a\mid s_t)=\nabla_\theta 1=0.$$

代回即得  
$$\mathbb E[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,X]=0.$$

</details>
<p></p>



因此

$$
\boxed{
\nabla_\theta J(\theta)
=\mathbb E_{\tau\sim p_\theta}\Big[\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)G_t\Big].
}
$$

这就是最常见的 REINFORCE 梯度表达。

### 2.4 REINFORCE 更新公式（蒙特卡洛估计）

用 $N$ 条独立轨迹采样估计上述期望：

$$
\hat g
=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_{t,n}\mid s_{t,n})G_{t,n},
\qquad
\theta\leftarrow\theta+\alpha\hat g.
$$

> 重要结论：REINFORCE 估计是 **无偏** 的（对真实 $\nabla J$ 的无偏估计），但通常 **方差很高**。

### 2.5 baseline：不改变期望、降低方差

对任意仅依赖状态的 $b(s)$：

$$
\mathbb E_{a\sim\pi_\theta(\cdot\mid s)}[\nabla_\theta\log\pi_\theta(a\mid s)b(s)]
=b(s)\sum_a \pi_\theta(a\mid s)\nabla_\theta\log\pi_\theta(a\mid s)
=b(s)\sum_a\nabla_\theta\pi_\theta(a\mid s)
=b(s)\nabla_\theta 1=0.
$$

因此可用 $G_t-b(s_t)$ 替代 $G_t$ 而不改变期望。

---

## 3. 策略梯度定理：从 Bellman 递归到占用测度 $\rho^\pi$

这一部分是我们讨论中“符号最多”的地方。核心思路是：

> $\nabla V^\pi$ 也满足一个 Bellman 型递归；
> 把这个递归展开后，自然出现折扣占用测度 $\rho^\pi$。

### 3.1 定义值函数与 Q 函数

$$
V^\pi(s)=\mathbb E_\pi\Big[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\mid s_0=s\Big],
\qquad
Q^\pi(s,a)=\mathbb E_\pi\Big[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\mid s_0=s,a_0=a\Big].
$$

目标函数可写为

$$
J(\theta)=\mathbb E_{s_0\sim\mu}[V^{\pi_\theta}(s_0)]=\sum_s\mu(s)V^{\pi_\theta}(s).
$$

### 3.2 策略诱导转移核 $P_\pi$

定义

$$
P_\pi(s'\mid s):=\sum_a\pi_\theta(a\mid s)P(s'\mid s,a).
$$

并用算子写法 $(P_\pi f)(s)=\sum_{s'}P_\pi(s'\mid s)f(s')$。

### 3.3 关键递归：$\nabla V^\pi$ 的 Bellman 型方程

Bellman 期望方程：

$$
V^\pi(s)=\sum_a\pi_\theta(a\mid s)Q^\pi(s,a).
$$

对 $\theta$ 求导：

$$
\nabla V^\pi(s)=\sum_a\nabla\pi_\theta(a\mid s)Q^\pi(s,a)+\sum_a\pi_\theta(a\mid s)\nabla Q^\pi(s,a).
$$

而

$$
Q^\pi(s,a)=r(s,a)+\gamma\sum_{s'}P(s'\mid s,a)V^\pi(s'),
$$

因此

$$
\nabla Q^\pi(s,a)=\gamma\sum_{s'}P(s'\mid s,a)\nabla V^\pi(s').
$$


代回得

$$\nabla V^\pi(s)=g_\theta(s)+\gamma\sum_{s'}\Big(\sum_a\pi_\theta(a\mid s)P(s'\mid s,a)\Big)\nabla V^\pi(s'),\quad g_\theta(s):=\sum_a \nabla\pi_\theta(a\mid s)\,Q^\pi(s,a).$$



即

$$
\boxed{
\nabla V^\pi = g_\theta + \gamma P_\pi(\nabla V^\pi).
}
$$

其中

$$
\boxed{
g_\theta(s)=\sum_a\nabla\pi_\theta(a\mid s),Q^\pi(s,a).
}
$$

### 3.4 展开递归：Neumann 级数与“未来影响链”

将上式改写为线性方程：

$$
(I-\gamma P_\pi)\nabla V^\pi=g_\theta.
$$

在适当条件下（例如有限状态且 $\gamma<1$），

$$
(I-\gamma P_\pi)^{-1}=\sum_{t=0}^{\infty}(\gamma P_\pi)^t.
$$

因此

$$
\boxed{
\nabla V^\pi=\sum_{t=0}^{\infty}\gamma^t P_\pi^t g_\theta.
}
$$

该式表达：$\nabla V$ 等于“立即贡献 $g$”加上“传到下一步、下下步……的累计影响”。

### 3.5 从 $\nabla V$ 推到 $\nabla J$：折扣占用测度出现

$$
\nabla J(\theta)=\sum_s\mu(s)\nabla V^\pi(s)
=\sum_s\mu(s)\sum_{t=0}^{\infty}\gamma^t (P_\pi^t g_\theta)(s).
$$

注意 $\mu P_\pi^t$ 给出在策略 $\pi$ 下从初始分布出发走 $t$ 步的状态边际分布。于是可写为

$$
\nabla J(\theta)=\sum_{t=0}^{\infty}\gamma^t\sum_s\Pr_\pi(s_t=s)g_\theta(s).
$$

定义（未归一化的）折扣状态占用测度：

$$
\boxed{
\rho^\pi(s):=\sum_{t=0}^{\infty}\gamma^t\Pr_\pi(s_t=s).
}
$$

于是

$$
\nabla J(\theta)=\sum_s\rho^\pi(s)g_\theta(s)
=\sum_s\rho^\pi(s)\sum_a\nabla\pi_\theta(a\mid s)Q^\pi(s,a).
$$

最后用 $\nabla\pi=\pi\nabla\log\pi$ 得

$$
\boxed{
\nabla_\theta J(\theta)
=\mathbb E_{s\sim\rho^\pi,\ a\sim\pi_\theta(\cdot\mid s)}\big[\nabla_\theta\log\pi_\theta(a\mid s)Q^\pi(s,a)\big].
}
$$

这就是策略梯度定理。

---
